对页表初始化完成之后，内核就可以对内存进行管理，但是内核并不是同一对待这些页面的，而是采用区块zone的方式来管理。`struct zone`数据结构主要成员如下

```

struct zone {
	/* Read-mostly fields */

	/* zone watermarks, access with *_wmark_pages(zone) macros */
	unsigned long watermark[NR_WMARK];

	/*
	 * We don't know if the memory that we're going to allocate will be freeable
	 * or/and it will be released eventually, so to avoid totally wasting several
	 * GB of ram we must reserve some of the lower zone memory (otherwise we risk
	 * to run OOM on the lower zones despite there's tons of freeable ram
	 * on the higher zones). This array is recalculated at runtime if the
	 * sysctl_lowmem_reserve_ratio sysctl changes.
	 */
	long lowmem_reserve[MAX_NR_ZONES];

#ifdef CONFIG_NUMA
	int node;
#endif

	/*
	 * The target ratio of ACTIVE_ANON to INACTIVE_ANON pages on
	 * this zone's LRU.  Maintained by the pageout code.
	 */
	unsigned int inactive_ratio;

	struct pglist_data	*zone_pgdat;
	struct per_cpu_pageset __percpu *pageset;

	/*
	 * This is a per-zone reserve of pages that should not be
	 * considered dirtyable memory.
	 */
	unsigned long		dirty_balance_reserve;

#ifndef CONFIG_SPARSEMEM
	/*
	 * Flags for a pageblock_nr_pages block. See pageblock-flags.h.
	 * In SPARSEMEM, this map is stored in struct mem_section
	 */
	unsigned long		*pageblock_flags;
#endif /* CONFIG_SPARSEMEM */

#ifdef CONFIG_NUMA
	/*
	 * zone reclaim becomes active if more unmapped pages exist.
	 */
	unsigned long		min_unmapped_pages;
	unsigned long		min_slab_pages;
#endif /* CONFIG_NUMA */

	/* zone_start_pfn == zone_start_paddr >> PAGE_SHIFT */
	unsigned long		zone_start_pfn;

	/*
	 * spanned_pages is the total pages spanned by the zone, including
	 * holes, which is calculated as:
	 * 	spanned_pages = zone_end_pfn - zone_start_pfn;
	 *
	 * present_pages is physical pages existing within the zone, which
	 * is calculated as:
	 *	present_pages = spanned_pages - absent_pages(pages in holes);
	 *
	 * managed_pages is present pages managed by the buddy system, which
	 * is calculated as (reserved_pages includes pages allocated by the
	 * bootmem allocator):
	 *	managed_pages = present_pages - reserved_pages;
	 *
	 * So present_pages may be used by memory hotplug or memory power
	 * management logic to figure out unmanaged pages by checking
	 * (present_pages - managed_pages). And managed_pages should be used
	 * by page allocator and vm scanner to calculate all kinds of watermarks
	 * and thresholds.
	 *
	 * Locking rules:
	 *
	 * zone_start_pfn and spanned_pages are protected by span_seqlock.
	 * It is a seqlock because it has to be read outside of zone->lock,
	 * and it is done in the main allocator path.  But, it is written
	 * quite infrequently.
	 *
	 * The span_seq lock is declared along with zone->lock because it is
	 * frequently read in proximity to zone->lock.  It's good to
	 * give them a chance of being in the same cacheline.
	 *
	 * Write access to present_pages at runtime should be protected by
	 * mem_hotplug_begin/end(). Any reader who can't tolerant drift of
	 * present_pages should get_online_mems() to get a stable value.
	 *
	 * Read access to managed_pages should be safe because it's unsigned
	 * long. Write access to zone->managed_pages and totalram_pages are
	 * protected by managed_page_count_lock at runtime. Idealy only
	 * adjust_managed_page_count() should be used instead of directly
	 * touching zone->managed_pages and totalram_pages.
	 */
	unsigned long		managed_pages;
	unsigned long		spanned_pages;
	unsigned long		present_pages;

	const char		*name;

	/*
	 * Number of MIGRATE_RESERVE page block. To maintain for just
	 * optimization. Protected by zone->lock.
	 */
	int			nr_migrate_reserve_block;

#ifdef CONFIG_MEMORY_ISOLATION
	/*
	 * Number of isolated pageblock. It is used to solve incorrect
	 * freepage counting problem due to racy retrieving migratetype
	 * of pageblock. Protected by zone->lock.
	 */
	unsigned long		nr_isolate_pageblock;
#endif

#ifdef CONFIG_MEMORY_HOTPLUG
	/* see spanned/present_pages for more description */
	seqlock_t		span_seqlock;
#endif

	/*
	 * wait_table		-- the array holding the hash table
	 * wait_table_hash_nr_entries	-- the size of the hash table array
	 * wait_table_bits	-- wait_table_size == (1 << wait_table_bits)
	 *
	 * The purpose of all these is to keep track of the people
	 * waiting for a page to become available and make them
	 * runnable again when possible. The trouble is that this
	 * consumes a lot of space, especially when so few things
	 * wait on pages at a given time. So instead of using
	 * per-page waitqueues, we use a waitqueue hash table.
	 *
	 * The bucket discipline is to sleep on the same queue when
	 * colliding and wake all in that wait queue when removing.
	 * When something wakes, it must check to be sure its page is
	 * truly available, a la thundering herd. The cost of a
	 * collision is great, but given the expected load of the
	 * table, they should be so rare as to be outweighed by the
	 * benefits from the saved space.
	 *
	 * __wait_on_page_locked() and unlock_page() in mm/filemap.c, are the
	 * primary users of these fields, and in mm/page_alloc.c
	 * free_area_init_core() performs the initialization of them.
	 */
	wait_queue_head_t	*wait_table;
	unsigned long		wait_table_hash_nr_entries;
	unsigned long		wait_table_bits;

	ZONE_PADDING(_pad1_)
	/* free areas of different sizes */
	struct free_area	free_area[MAX_ORDER];

	/* zone flags, see below */
	unsigned long		flags;

	/* Write-intensive fields used from the page allocator */
	spinlock_t		lock;

	ZONE_PADDING(_pad2_)

	/* Write-intensive fields used by page reclaim */

	/* Fields commonly accessed by the page reclaim scanner */
	spinlock_t		lru_lock;
	struct lruvec		lruvec;

	/* Evictions & activations on the inactive file list */
	atomic_long_t		inactive_age;

	/*
	 * When free pages are below this point, additional steps are taken
	 * when reading the number of free pages to avoid per-cpu counter
	 * drift allowing watermarks to be breached
	 */
	unsigned long percpu_drift_mark;

#if defined CONFIG_COMPACTION || defined CONFIG_CMA
	/* pfn where compaction free scanner should start */
	unsigned long		compact_cached_free_pfn;
	/* pfn where async and sync compaction migration scanner should start */
	unsigned long		compact_cached_migrate_pfn[2];
#endif

#ifdef CONFIG_COMPACTION
	/*
	 * On compaction failure, 1<<compact_defer_shift compactions
	 * are skipped before trying again. The number attempted since
	 * last failure is tracked with compact_considered.
	 */
	unsigned int		compact_considered;
	unsigned int		compact_defer_shift;
	int			compact_order_failed;
#endif

#if defined CONFIG_COMPACTION || defined CONFIG_CMA
	/* Set to true when the PG_migrate_skip bits should be cleared */
	bool			compact_blockskip_flush;
#endif

	ZONE_PADDING(_pad3_)
	/* Zone statistics */
	atomic_long_t		vm_stat[NR_VM_ZONE_STAT_ITEMS];
} ____cacheline_internodealigned_in_smp;
```

首先`struct zone`是经常被访问到的，因此这个数据结构要求以L1 cache对齐。另外，这里的`ZONE_PADDING()`是让zone->lock和`zone->lru_lock`这两个很热门的锁可以分布在不同的cache line中。一个内存节点最多也就几个zone，因此zone数据结构不需要像struct page一样关注数据结构的大小，因此这里ZONE_PADDING()可以为了性能而浪费空间。在内存管理开发过程中，内核开发者逐步发现一些自旋锁会竞争的很厉害，很难获取。像`zone->lock`和`zone->lru_lock`这两个锁有时需要同时获取锁，因此保证它们使用不同的cache line是内核常用的一种优化技巧；

- watermark：每个zone在系统启动时会计算出3个水位值，分别是WMARK_MIN、WMARK_LOW和WMARK_HIGH水位，这在页面分配器和kswapd页面回收中会用到；
- `lowmem_reserve`：zone中预留的内存
- `zone_pgdat`：指向内存节点
- `pageset`：用于维护Pre-CPU上的一系列的页面，以减少自旋锁的争用；
- `zone_start_pfn`：zone中被伙伴系统管理的页面数量；
- `managed_pages`：zone中被伙伴系统管理的页面数量；
- `spanned_pages`:zone包含的页面数量
- `present_pages`：zone里实际管理的页面数量。对一些体系结构来说，其值和`spanned_pages`相等
- `free_area`：管理空闲区域的数组，包含管理链表等；
- `lock`：并行访问zone，对zone保护的自旋锁
- `lru_lock`：用于zone中LRU链表并行访问时进行保护的自旋锁。
- `lruvec`：LRU链表集合
- `vm_stat`：zone计数

通常情况下，内核的zone分为ZONE_DMA、ZONE_DMA32、ZONE_NORMAL和ZONE_HIGHMEM。在ARM Vexpress平台中，没有定义CONFIG_ZONE_DMA和CONFIG_ZONE_DMA32,所以只有ZONE_NORMAL和ZONE_HIGHMEM两种；zone的定义在`include/linux/mmzone.h`文件中：

```c

enum zone_type {
#ifdef CONFIG_ZONE_DMA
	/*
	 * ZONE_DMA is used when there are devices that are not able
	 * to do DMA to all of addressable memory (ZONE_NORMAL). Then we
	 * carve out the portion of memory that is needed for these devices.
	 * The range is arch specific.
	 *
	 * Some examples
	 *
	 * Architecture		Limit
	 * ---------------------------
	 * parisc, ia64, sparc	<4G
	 * s390			<2G
	 * arm			Various
	 * alpha		Unlimited or 0-16MB.
	 *
	 * i386, x86_64 and multiple other arches
	 * 			<16M.
	 */
	ZONE_DMA,
#endif
#ifdef CONFIG_ZONE_DMA32
	/*
	 * x86_64 needs two ZONE_DMAs because it supports devices that are
	 * only able to do DMA to the lower 16M but also 32 bit devices that
	 * can only do DMA areas below 4G.
	 */
	ZONE_DMA32,
#endif
	/*
	 * Normal addressable memory is in ZONE_NORMAL. DMA operations can be
	 * performed on pages in ZONE_NORMAL if the DMA devices support
	 * transfers to all addressable memory.
	 */
	ZONE_NORMAL,
#ifdef CONFIG_HIGHMEM
	/*
	 * A memory area that is only addressable by the kernel through
	 * mapping portions into its own address space. This is for example
	 * used by i386 to allow the kernel to address the memory beyond
	 * 900MB. The kernel will set up special mappings (page
	 * table entries on i386) for each page that the kernel needs to
	 * access.
	 */
	ZONE_HIGHMEM,
#endif
	ZONE_MOVABLE,
	__MAX_NR_ZONES
};
```

zone的初始化函数集中在`bootmem_init()`中完成，所以需要确定每个zone的范围。在find_limit()函数中会计算出min_low_pf、max_low_pfn和max_pfn这三个值。其中，min_low_pfn是内存块的开始地址的页帧号(0x60000)，max_low_pfn(0x8f800)表示normal区域的结束页帧号。



下面是ARM Vexpress平台运行之后的打印出来的zone信息：

![zone信息图](./picture/zone信息图.png)

可以看出ARM Vexpress平台分为两个zone，ZONE_NORMAL和ZONE_HIGHMEM。其中ZONE_NORMAL是从0xc0000000到0xef800000，这个地址空间有多少页面呢？

> (0xef800000-0xc0000000)/4096 = 194560

所以ZONE_NORMAL有194560个页面；

另外ZONE_NORMAL的虚拟地址的结束地址是0xef800000，减去PAGE_OFFSET(0xc0000000)，再加上PHY_OFFSET(0x60000000)，正好等于0x8f800000，这个值等于我们之前计算出的arm_lowmem_init。

> PHYS_OFFSET为由于Vexpress的RAM起始地址映射在0x60000000。所以要加上这一部分。

```static inline phys_addr_t __virt_to_phys(unsigned long x)
static inline phys_addr_t __virt_to_phys(unsigned long x)
{
    return (phys_addr_t)x - PAGE_OFFSET + PHYS_OFFSET;
}

static inline unsigned long __phys_to_virt(phys_addr_t x)
{
    return x - PHYS_OFFSET + PAGE_OFFSET;
}
```

ZONE_NORMAL大小760MB，从0xc0000000 - 0xef800000，ZONE_HIGHMEM大小264MB，从0xef800000 - 0xffffffff。

ZONE_HIGHMEM并不等同于vmalloc，还有8MB hole和末尾16MB空间。所以vmalloc=264-8-16=240MB。



zone的初始化在free_area_init_core()中：

```
start_kernel	-->
​	setup_arch	-->
​		paging_init	-->
​			bootmem_init	-->
				zone_sizes_init	-->
					free_area_init_node	-->
						free_area_init_core
```

```/*
 * Set up the zone data structures:
 *   - mark all pages reserved
 *   - mark all memory queues empty
 *   - clear the memory bitmaps
 *
 * NOTE: pgdat should get zeroed by caller.
 */
static void __paginginit free_area_init_core(struct pglist_data *pgdat,
		unsigned long node_start_pfn, unsigned long node_end_pfn,
		unsigned long *zones_size, unsigned long *zholes_size)
{
	enum zone_type j;
	int nid = pgdat->node_id;
	unsigned long zone_start_pfn = pgdat->node_start_pfn;
	int ret;

	pgdat_resize_init(pgdat);
#ifdef CONFIG_NUMA_BALANCING
	spin_lock_init(&pgdat->numabalancing_migrate_lock);
	pgdat->numabalancing_migrate_nr_pages = 0;
	pgdat->numabalancing_migrate_next_window = jiffies;
#endif
	//初始化内存节点的内存置换等待队列。
	init_waitqueue_head(&pgdat->kswapd_wait);
	//pfmemalloc_wait等待队列是用来由kswapd唤醒后进行内存分配的
	init_waitqueue_head(&pgdat->pfmemalloc_wait);
	pgdat_page_ext_init(pgdat);

	/* 遍历每个管理区 */
	for (j = 0; j < MAX_NR_ZONES; j++) {
		struct zone *zone = pgdat->node_zones + j;
		unsigned long size, realsize, freesize, memmap_pages;
		/*  size为该管理区中的页框数，包括洞，该函数计算区域的的包含的页面数，包含旁边可能存在的空泛。计算区域总页面数要斟酌两个因数： */
		size = zone_spanned_pages_in_node(nid, j, node_start_pfn,
						  node_end_pfn, zones_size);
		 /* realsize为管理区中的页框数，不包括洞  /
		realsize = freesize = size - zone_absent_pages_in_node(nid, j,
								node_start_pfn,
								node_end_pfn,
								zholes_size);

		/*
		 * Adjust freesize so that it accounts for how much memory
		 * is used by this zone for memmap. This affects the watermark
		 * and per-cpu initialisations
		 * 调整realsize的大小，即减去page结构体占用的内存大小 
		 */
		 // memmap_pags为包括洞的所有页框的page结构体所占的大小
		memmap_pages = calc_memmap_size(size, realsize);
		if (!is_highmem_idx(j)) {
			if (freesize >= memmap_pages) {
				freesize -= memmap_pages;
				if (memmap_pages)
					printk(KERN_DEBUG
					       "  %s zone: %lu pages used for memmap\n",
					       zone_names[j], memmap_pages);
			} else /*  内存不够存放page结构体  */
				printk(KERN_WARNING
					"  %s zone: %lu pages exceeds freesize %lu\n",
					zone_names[j], memmap_pages, freesize);
		}

		/* Account for reserved pages */
		//调整realsize的大小，即减去DMA保留页的大小
		if (j == 0 && freesize > dma_reserve) {
			freesize -= dma_reserve;
			printk(KERN_DEBUG "  %s zone: %lu pages reserved\n",
					zone_names[0], dma_reserve);
		}
		//HIGHMEM不计算映射耗费page数目
		if (!is_highmem_idx(j))
			nr_kernel_pages += freesize;
		/* Charge for highmem memmap if there are enough kernel pages */
		else if (nr_kernel_pages > memmap_pages * 2)
			nr_kernel_pages -= memmap_pages;
		nr_all_pages += freesize;

		zone->spanned_pages = size;
		zone->present_pages = realsize;
		/*
		 * Set an approximate value for lowmem here, it will be adjusted
		 * when the bootmem allocator frees pages into the buddy system.
		 * And all highmem pages will be managed by the buddy system.
		 */
		  /* 设置zone->spanned_pages为包括洞的页框数  */
		zone->managed_pages = is_highmem_idx(j) ? realsize : freesize;
#ifdef CONFIG_NUMA
		/* 设置zone中的节点标识符 */
		zone->node = nid;
		 /* 设置可回收页面比率 */
		zone->min_unmapped_pages = (freesize*sysctl_min_unmapped_ratio)
						/ 100;
		/* 设置slab回收缓存页的比率 */
		zone->min_slab_pages = (freesize * sysctl_min_slab_ratio) / 100;
#endif
		 /*  设置zone的名称  */
		zone->name = zone_names[j];
		/* 初始化各种锁 */
		spin_lock_init(&zone->lock);
		spin_lock_init(&zone->lru_lock);
		zone_seqlock_init(zone);
		/* 设置管理区属于的节点对应的pg_data_t结构 */
		zone->zone_pgdat = pgdat;
		 /* 初始化cpu的页面缓存 */
		zone_pcp_init(zone);

		/* For bootup, initialized properly in watermark setup */
		mod_zone_page_state(zone, NR_ALLOC_BATCH, zone->managed_pages);

		/* 初始化lru相关成员 */
		lruvec_init(&zone->lruvec);
		if (!size)
			continue;

		set_pageblock_order();
		/* 定义了CONFIG_SPARSEMEM该函数为空 */
		setup_usemap(pgdat, zone, zone_start_pfn, size);
		/* 设置pgdat->nr_zones和zone->zone_start_pfn成员
         * 初始化zone->free_area成员
         * 初始化zone->wait_table相关成员
         */
		ret = init_currently_empty_zone(zone, zone_start_pfn,
						size, MEMMAP_EARLY);
		BUG_ON(ret);
		 /* 初始化该zone对应的page结构 */
		memmap_init(size, nid, j, zone_start_pfn);
		zone_start_pfn += size;
	}
}
```

另外系统中会有一个zonelist的数据结构，伙伴系统分配器会从zonelist开始分配内存，zonelist有一个zoneref数组，数组里有一个成员会指向zone数据结构。zoneref数组的第一个成员指向的zone是页面分配器的第一个候选者，其他成员则是第一个候选者分配失败之后才考虑，优先级逐渐减低，zonelist的初始化路径如下：

```
start_kernel->build_all_zonelists
				->	build_all_zonelists_init
					-> __build_all_zonelists
						-> build_zonelists
							->	build_zonelists_node
```

```/*
 * Builds allocation fallback zone lists.
 *
 * Add all populated zones of a node to the zonelist.
 */
static int build_zonelists_node(pg_data_t *pgdat, struct zonelist *zonelist,
				int nr_zones)
{
	struct zone *zone;
	enum zone_type zone_type = MAX_NR_ZONES;

	do {
		zone_type--;
		zone = pgdat->node_zones + zone_type;
		if (populated_zone(zone)) {
			zoneref_set_zone(zone,
				&zonelist->_zonerefs[nr_zones++]);
			check_highest_zone(zone_type);
		}
	} while (zone_type);

	return nr_zones;
}
```

这里从最高的MAX_NR_ZONES的zone开始，设置到_zonerefs[0]数组中。在ARM Vexpress平台上，该函数的运行结果如下：

> HighMem		_zonerefs[0]->zone_index=1
>
> Normal			_zonerefs[1]->zone_index=0

另外，系统中还有一个非常重要的全局变量-mem_map，它是一个struct page的数组，可以实现快速的把虚拟地址映射到物理地址中，这里指的是内核地址的线性映射，它的初始化是在free_area_init_node()->alloc_node_mem_map()函数中；