释放slab缓冲对象的API函数是kmem_cache_free()函数。

```
/**
 * kmem_cache_free - Deallocate an object
 * @cachep: The cache the allocation was from.
 * @objp: The previously allocated object.
 *
 * Free an object which was previously allocated from this
 * cache.
 */
void kmem_cache_free(struct kmem_cache *cachep, void *objp)
{
	unsigned long flags;
	cachep = cache_from_obj(cachep, objp);
	if (!cachep)
		return;

	local_irq_save(flags);
	debug_check_no_locks_freed(objp, cachep->object_size);
	if (!(cachep->flags & SLAB_DEBUG_OBJECTS))
		debug_check_no_obj_freed(objp, cachep->object_size);
	__cache_free(cachep, objp, _RET_IP_);
	local_irq_restore(flags);

	trace_kmem_cache_free(_RET_IP_, objp);
}
```

首先，cache_from_obj()通过要释放对象的obj的虚拟地址找到对应的struct kmem_cache数据结构。由对象的虚拟地址通过virt_to_pfn()找到相应的pfn，然后通过pfn_to_page()由pfn找到对应的page结构。在这个slab中，第一个页面page结构中page->slab_cache指向这个struct kmem_cache数据结构。

```
#define virt_to_page(kaddr)	pfn_to_page(__pa(kaddr) >> PAGE_SHIFT)
static inline struct page *virt_to_head_page(const void *x)
{
	struct page *page = virt_to_page(x);

	/*
	 * We don't need to worry about synchronization of tail flag
	 * when we call virt_to_head_page() since it is only called for
	 * already allocated page and this page won't be freed until
	 * this virt_to_head_page() is finished. So use _fast variant.
	 */
	return compound_head_fast(page);
}
static inline struct kmem_cache *cache_from_obj(struct kmem_cache *s, void *x)
{
	struct kmem_cache *cachep;
	struct page *page;

	/*
	 * When kmemcg is not being used, both assignments should return the
	 * same value. but we don't want to pay the assignment price in that
	 * case. If it is not compiled in, the compiler should be smart enough
	 * to not do even the assignment. In that case, slab_equal_or_root
	 * will also be a constant.
	 */
	if (!memcg_kmem_enabled() && !unlikely(s->flags & SLAB_DEBUG_FREE))
		return s;

	page = virt_to_head_page(x);
	cachep = page->slab_cache;
	if (slab_equal_or_root(cachep, s))
		return cachep;

	pr_err("%s: Wrong slab cache. %s but object is from %s\n",
	       __func__, cachep->name, s->name);
	WARN_ON_ONCE(1);
	return s;
}
```

kmem_cache_free()函数第5行代码关闭本地CPU中断。

```
[kmem_cache_free()->__cache_free()]

/*
 * Release an obj back to its cache. If the obj has a constructed state, it must
 * be in this state _before_ it is released.  Called with disabled ints.
 */
static inline void __cache_free(struct kmem_cache *cachep, void *objp,
				unsigned long caller)
{
	struct array_cache *ac = cpu_cache_get(cachep);

	check_irq_off();
	kmemleak_free_recursive(objp, cachep->flags);
	objp = cache_free_debugcheck(cachep, objp, caller);

	kmemcheck_slab_free(cachep, objp, cachep->object_size);

	/*
	 * Skip calling cache_free_alien() when the platform is not numa.
	 * This will avoid cache misses that happen while accessing slabp (which
	 * is per page memory  reference) to get nodeid. Instead use a global
	 * variable to skip the call, which is mostly likely to be present in
	 * the cache.
	 */
	if (nr_online_nodes > 1 && cache_free_alien(cachep, objp))
		return;

	if (ac->avail < ac->limit) {
		STATS_INC_FREEHIT(cachep);
	} else {
		STATS_INC_FREEMISS(cachep);
		cache_flusharray(cachep, ac);
	}

	ac_put_obj(cachep, ac, objp);
}
```

第11行代码，ac_put_obj()中的"ac->entry[ac->avail++] = objp"把对象释放到本地对象缓冲池ac中，释放过程已经结束了。

如果考虑第5行代码的判断条件，但本地对象缓冲池的空闲对象ac->avail大于ac->limit的阈值时，就会调用cache_flusharray()做flush动作去尝试回收空闲对象。ac->limit阈值的计算在enable_cpucache()函数中进行，在我们例子中，ac->limit为120，ac->batchcount为60；

```
[kmem_cache_free()->__cache_free()->cache_flusharray()]
static void cache_flusharray(struct kmem_cache *cachep, struct array_cache *ac)
{
	int batchcount;
	struct kmem_cache_node *n;
	int node = numa_mem_id();
	LIST_HEAD(list);

	batchcount = ac->batchcount;
#if DEBUG
	BUG_ON(!batchcount || batchcount > ac->avail);
#endif
	check_irq_off();
	n = get_node(cachep, node);
	spin_lock(&n->list_lock);
	if (n->shared) {
		struct array_cache *shared_array = n->shared;
		int max = shared_array->limit - shared_array->avail;
		if (max) {
			if (batchcount > max)
				batchcount = max;
			memcpy(&(shared_array->entry[shared_array->avail]),
			       ac->entry, sizeof(void *) * batchcount);
			shared_array->avail += batchcount;
			goto free_done;
		}
	}

	free_block(cachep, ac->entry, batchcount, node, &list);
free_done:
#if STATS
	{
		int i = 0;
		struct list_head *p;

		p = n->slabs_free.next;
		while (p != &(n->slabs_free)) {
			struct page *page;

			page = list_entry(p, struct page, lru);
			BUG_ON(page->active);

			i++;
			p = p->next;
		}
		STATS_SET_FREEABLE(cachep, i);
	}
#endif
	spin_unlock(&n->list_lock);
	slabs_destroy(cachep, &list);
	ac->avail -= batchcount;
	memmove(ac->entry, &(ac->entry[batchcount]), sizeof(void *)*ac->avail);
}
```

在cache_flusharray()函数中，首先判断是否有共享对象缓冲池，如果有，第10~19行代码就会把本地对象缓冲池的空闲对象复制到共享对象缓冲池中，这里复制batchcount个空闲对象。第28行代码是本地对象缓冲池剩余空闲对象前移到buffer的头部。

假设共享对象缓冲池中的空闲对象数量大于limit阈值，那么就会跑到第23行代码中的free_block()函数中，free_block()函数中，free_block()函数会主动释放batchcount个空闲对象。如果slab没有了活跃对象（即page->active==0）,并且slab节点上所有空闲对象数目n->free_objects超过了n->free_limit阈值，那么调用slabs_destory()函数来销毁这个slab。page->active用于记录活跃的slab对象的计数，slab_get_obj()函数分配一个slab对象时会增加该计数，slab_put_obj()函数会释放这个slab对象会递减这个计数。

```
static void free_block(struct mem_block *p)
{
	p->file_priv = NULL;

	/* Assumes a single contiguous range.  Needs a special file_priv in
	 * 'heap' to stop it being subsumed.
	 */
	if (p->next->file_priv == NULL) {
		struct mem_block *q = p->next;
		p->size += q->size;
		p->next = q->next;
		p->next->prev = p;
		kfree(q);
	}

	if (p->prev->file_priv == NULL) {
		struct mem_block *q = p->prev;
		q->size += p->size;
		q->next = p->next;
		q->next->prev = q;
		kfree(p);
	}
}
```

